## A word embedding-based approach to cross-lingual topic modeling  

#### 摘要：

早期方法 依赖于多语言资源--平行语料库  难以获得

依赖翻译字典     当字典覆盖面小   跨语言主题模型退化为单语言主题模型

最近 一些空间映射技术被提出，以帮助对齐不同语言的多个词嵌入，通过参考少量的翻译对来构建高质量的跨语言词嵌入。

本文提出Cb-CLTM跨语言主题模型，结合跨语言词嵌入，将每个词视为一个连续的嵌入向量，而不是离散的词类型。  

#### 相关工作：

##### 跨语言LDA：

1.文档链接 

  PLTM 假设每个元组中的文档共享相同的主题分布，并且每种语言中的每个主题都有特定的词语分布。 文档链接模型需要平行语料库或可比语料库，这在许多情况下可能无法获得。

2.词汇链接

依赖于使用翻译字典，词典规模的限制通常会将词汇链接模型缩减为单一语言的主题模型 。

为了缓解这些限制，我们的工作将词典条目视为不同语言连续词空间中的锚点，而不是主题分布中的可能值。得益于跨语言词嵌入技术 ，我们可以通过少量词典条目获得高质量的跨语言词空间。

##### 连续LDA：

连续主题模型都主要应用于单语言语料库，如何将它们扩展到跨语言应用仍是一个挑战。

PMLDA 通过跨语言词向量空间将多个单语言主题模型连接成跨语言主题模型，并使用DBSCAN算法对主题进行分组。

基于Transformer的语言模型（如M-BERT）通过大型多语言语料库直接学习跨语言词表示。ZeroShotTM 模型则结合M-BERT的多语言能力，通过推理网络和解码网络生成跨语言的主题模型，但其解码网络在跨语言主题解释性方面存在一定限制。

#### Cb-CLTM

基于LDA ，语料库D由不同语言的文档组成。每个文档被标记为其对应的语言（ld）。

文档-主题分布: 表示每个文档中传达的主题倾向。 

主题-词分布 : 收集与主题相关的不同语言中具有相似上下文的词语。

##### 跨语言词嵌入准备：

​	首先构建单语言词空间，使用skip-garm算法训练单语言词向量。

​	采用正交变换方法，将俩个单语言空间对齐成一个跨语言词空间。

![image-20241020111457724](C:\Users\wdx\AppData\Roaming\Typora\typora-user-images\image-20241020111457724.png)

如图，展示了对齐过程，显示了源语言 l 和目标语言 l′ 的预训练词空间。 给定(科学，science)等词对，可以确定转换矩阵W，以便将源词空间旋转到目标词空间。

##### 基于中心的跨语言主题模型：

  	提出了一种名为基于中心的跨语言主题模型（Cb-CLTM）的方法，词向量被视为生成过程中新的观测变量。为了整合跨语言词嵌入，我们将主题-词的分类分布替换为以词嵌入形式存在的主题中心。

1. 每个主题t被表示为跨语言词空间中的一个多元向量ψt ，视为主题的语义中心

2. 主题-词分布，通过softmax函数近似每个主题的词分类分布ϕt，这意味着如果一个词的跨语言词向量接近某个主题的中心 ψt，它被选择的概率就更高。

   ![image-20241020115308974](C:\Users\wdx\AppData\Roaming\Typora\typora-user-images\image-20241020115308974.png)

##### 降低嵌入的语言维度：

​	出现的问题：

​		偏向特定语言：当使用预训练的跨语言词向量时，生成的主题可能会偏向某种特定语言。这是因为在跨语言词空间中，某些维度可能只反映某种语言的特征。

​		按语言聚类：由于这种偏向，靠近语义中心的词在特定的语言维度上往往具有相似值，从而导致相同语言的词聚集在一起，而不是跨语言的相关词。

 	解决方法：

​		消除那些与特定语言相关的维度。

​		移除那些预测能力高的维度被认为是语言特定的。

在移除语言特定维度后，对每个嵌入的行应用L2正则化进行归一化处理。这一步确保了不同语言之间的词向量能够在一个统一的标准下进行比较。

移除的维度数量作为超参数。

![image-20241020121034985](C:\Users\wdx\AppData\Roaming\Typora\typora-user-images\image-20241020121034985.png)

#### 总结模型流程：

##### 1. 数据准备

- **文本处理**：首先，对不同语言的文档进行文本处理，提取标记和词性标签。去除停用词，并保留名词和动词，以便训练单语词向量。
- **单语词向量训练**：使用跳字模型（Skip-gram）和负采样技术，从处理后的文本中生成单语词向量。

##### 2. 生成跨语言词嵌入

- **构建双语词典**：使用双语词典来对齐不同语言的单语词向量空间。通过正交变换方法，生成一个跨语言的词嵌入空间。
- **对齐词向量**：利用双语词典中提供的翻译词对，通过最小二乘法训练变换矩阵，将一个语言的词向量空间映射到另一个语言的词向量空间。

##### 3. 生成过程

- **初始化主题**：为每个文档 d初始化主题分布 θd 和主题中心 ψt
- **主题中心表示**：用词嵌入表示每个主题的中心 ψt，而不是使用传统的主题-词分布。这种方式降低了模型的参数数量和计算复杂度。

##### 4. 参数推断

- **Gibbs采样**：使用Gibbs采样方法推断文档的主题分布 θ和主题中心 ψ。通过样本的主题分配，迭代更新主题分布。
- **更新主题向量**：通过最大似然估计（MLE）来更新每个主题的中心 ψt。

##### 5. 语言维度降维

- **识别语言特定维度**：使用回归算法等方法识别语言特定的维度，并将其去除。去除这些维度后，通过L2归一化来标准化词向量。
- **超参数调整**：控制去除的维度数量，以在跨语言语义完整性和模型性能之间进行权衡。

##### 6. 迭代优化

- **迭代更新**：在每次迭代中优化主题中心 ψt，更新文档主题分布 θd，并重新分配词的主题索引，直到达到收敛。

##### 7. 输出结果

- **提取主题**：最终得到每个主题的中心及其相关的词向量，可以用来分析不同语言之间的主题模式和关联。

### Experimental results

##### 数据集：

UM-Corpus 是一个平行语料库，包含大量的英文和中文句子对。

Reuters Corpus Volume 2 是一个非平行和不可比较的语料库，包含 13 种语言的众多新闻文章。

使用了来自 Facebook MUSE 项目的中英和日英双语词典，该词典可在 [GitHub](https://github.com/facebookresearch/MUSE) 上获得。UM-Corpus 和 RCV2 中的中英词典覆盖率分别为 8.7% 和 4.6%。日英词典在 RCV2 中覆盖了 7.2% 的词汇。

##### 评估指标:

1.一致性指标 (Coherence Metric)

NPMI  归一化点互信息 

CNPMl 跨语言NPMI

2.多样性指标 (Diversity Metric)

Inverse Jaccard Index：评估主题模型中主题的多样性，越高的逆Jaccard指数表明生成的主题越多样化。

3.跨语言文档表示质量指标 (Metric for the Quality of Cross-Lingual Document Representation)

JSD：用于评估并行数据集中推断的文档-主题分布 (θdl,θdl′)之间的差异性。报告时使用逆JSD，即 1−JSD，分数越高表示主题分布越相似。

#### 对比模型：

**PLTM**：是一种代表性的文档链接模型。它需要平行或可比较的语料库作为输入，并假设同一对文档共享相同的主题分布。

**JointLDA**：是一种经过充分研究的词汇链接模型。该模型将双语词典中的每个条目表示为主题-词分布中的一个词概念，以捕捉跨语言主题。

**PMLDA**：同样使用跨语言词空间来连接跨语言的主题。它首先确定单语主题，然后通过聚类构建跨语言主题，以将具有语义意义的主题链接在一起。

**MTAnchor**：是基于锚点的主题模型的多语言扩展。当给定双语词典时，它首先通过在低维词空间中搜索凸包来从词典中找到双语主题锚点。然后，通过 RecoverL2 算法恢复主题-词分布。

#### 实验比较总结：

##### **模型比较**：

- Cb-CLTM 在 UM-Corpus 数据集中表现优异，尤其在 UM-Corpus 25K 上的 CNPMI 分数优于其他模型，表明其在生成跨语言主题时的有效性。
- PMLDA 在 UM-Corpus 上表现良好，但在 MLDoc 数据集上未能有效生成跨语言主题，主要是由于字典覆盖率低，导致仅生成单语主题。
- PLTM 在短文本特征下表现最差，因其假设每对文档共享相同的主题分布，影响了其性能。
- MTAnchor 由于重复词汇和手动选择过程，生成的主题多样性最差。

##### **主题一致性**：

- Cb-CLTM 和 PMLDA 在相同的跨语言词空间下表现相近，但 Cb-CLTM 在小数据集（UM-Corpus 25K）上生成了更一致的主题。
- PLTM 在技术主题上的一致性较差，生成的词汇与主题无关，进一步证实其为最不连贯的模型。

##### **主题多样性**：

- Cb-CLTM 在所有数据集中都表现出较小的标准差，表明其生成的主题具有更高的多样性。
- 其他模型如 PLTM 和 JointLDA 由于高频词主导，导致主题多样性较低。

##### **跨语言文档表示性能**：

- Cb-CLTM 在大多数设置下表现突出，特别是在文档-主题分布 θ 的生成方面，促进了更好的跨语言文档表示。
- PLTM 随着主题数量的增加，其逆 JSD 值降低，表明其在处理多个主题时性能不佳。

##### **定性分析**：

- Cb-CLTM、PMLDA 和 JointLDA 在主题-词分布上生成了合理的结果，而 PLTM 在技术主题上的表现较差。
- 在 MLDoc 数据集中，PMLDA 生成的主题缺乏完全的跨语言特征，MTAnchor 则由于重复主题导致多样性下降。

### 总结

​	本文提出了一种跨语言主题模型——Cb-CLTM，它通过利用跨语言词嵌入来扩展单语 LDA，从而推断跨语言主题。在平行语料库 UM-Corpus 和 UM-Corpus 25K 中，我们发现 Cb-CLTM 在大多数设置下的所有指标上均优于其他模型，这表明跨语言词嵌入所表示的词语语义关系确实有助于构建更好的跨语言主题模型。Cb-CLTM 不需要平行/可比语料库，仅依赖于少量的双语词典条目。在较少的词典条目情况下，Cb-CLTM 在生成一致的主题、产生多样的主题和学习跨语言文档表示方面优于 JointLDA 和 MTAnchor。

​	对于非平行语料库 MLDoc En-Zh 和 MLDoc En-Ja，由于原始 RCV2 语料库中各语言文章主题的分布非常不同，这导致生成的跨语言词空间在结构上缺乏同构性。在输入非对齐的跨语言词空间时，Cb-CLTM 的一致性表现较低，但在主题多样性和零样本跨语言文档分类方面仍然表现优异。

​	